---
title: "Evaluate your model with resampling"
date: last-modified
date-format: "[Last modified on] MMMM DD, YYYY HH:mm:ss zzz"
format:
  html: 
    axe:
     output: document
  pdf: default
editor: source
---

```{r include = FALSE}
library(knitr) # packages
library(tidyverse)
library(scales)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", comment = NA, message = TRUE,  warning = TRUE)
```

:::{.callout-important title="Crediting the materials" icon=false}
The majority of the material in this document is taken from [https://www.tidymodels.org/start/models/](https://www.tidymodels.org/start/models/)
:::

## INTRODUCTION

So far, we have built a model and preprocessed data with a recipe. We also introduced workflows as a way to bundle a [parsnip](https://parsnip.tidymodels.org/) model and [recipe](https://recipes.tidymodels.org/) together. Once we have a model trained, we need a way to measure how well that model predicts new data. This tutorial explains how to characterize model performance based on **resampling** statistics.

To use code in this article, you will need to install the following packages: `modeldata`, `ranger`, and `tidymodels`.

:::{.callout-note icon=false}
All of the required packages are already **installed** on the mathr server with the exception of `modeldata` which you can install to your User Library using `install.packages("modeldata")`.  You will need to **load** the packages using the `library()` function to access the data and functions in each package.
:::

```{r, message = FALSE}
library(tidymodels) # for the rsample package, along with the rest of tidymodels

# Helper packages
library(modeldata)  # for the cells data
```

## THE CELL IMAGE DATA

Let's use data from [Hill, LaPan, Li, and Haney (2007)](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340), available in the `modeldata` package, to predict cell image segmentation quality with resampling.  To start, we load this data into R:

```{r}
data(cells, package = "modeldata")
cells
```

We have data for 2019 cells, with 58 variables. The main outcome variable of interest for us here is called `class`, which you can see is a factor. But before we jump into predicting the class variable, we need to understand it better. Below is a brief primer on cell image segmentation.

## Predicting image segmentation quality

Some biologists conduct experiments on cells. In drug discovery, a particular type of cell can be treated with either a drug or control and then observed to see what the effect is (if any). A common approach for this kind of measurement is cell imaging. Different parts of the cells can be colored so that the locations of a cell can be determined.

For example, in top panel of this image of five cells, the green color is meant to define the boundary of the cell (coloring something called the cytoskeleton) while the blue color defines the nucleus of the cell.

```{r, echo = FALSE}
knitr::include_graphics("./jpg/cell.jpg")
```

Using these colors, the cells in an image can be _segmented_ so that we know which pixels belong to which cell. If this is done well, the cell can be measured in different ways that are important to the biology. Sometimes the shape of the cell matters and different mathematical tools are used to summarize characteristics like the size or “oblongness” of the cell.

The bottom panel shows some segmentation results. Cells 1 and 5 are fairly well segmented. However, cells 2 to 4 are bunched up together because the segmentation was not very good. The consequence of bad segmentation is data contamination; when the biologist analyzes the shape or size of these cells, the data are inaccurate and could lead to the wrong conclusion.

A cell-based experiment might involve millions of cells so it is unfeasible to visually assess them all. Instead, a subsample can be created and these cells can be manually labeled by experts as either poorly segmented (`PS`) or well-segmented (`WS`). If we can predict these labels accurately, the larger data set can be improved by filtering out the cells most likely to be poorly segmented.

## Back to the cells data

The `cells` data has `class` labels for 2019 cells — each cell is labeled as either poorly segmented (`PS`) or well-segmented (`WS`)`. Each also has a total of 56 predictors based on automated image analysis measurements. For example, `avg_inten_ch_1` is the mean intensity of the data contained in the nucleus, `area_ch_1` is the total size of the cell, and so on (some predictors are fairly arcane in nature).

```{r}
cells
```

The rates of the classes are somewhat imbalanced; there are more poorly segmented cells than well-segmented cells:

```{r}
cells  |>  
  count(class)  |>  
  mutate(prop = n/sum(n)) |> 
  kable()
```

## DATA SPLITTING

In our previous _Preprocess your data with recipes_ article, we started by splitting our data. It is common when beginning a modeling project to separate the data set into two partitions:

* The _**training set**_ is used to estimate parameters, compare models and feature engineering techniques, tune models, etc.

* The _**test set**_ is held in reserve until the end of the project, at which point there should only be one or two models under serious consideration. It is used as an unbiased source for measuring final model performance.

There are different ways to create these partitions of the data. The most common approach is to use a random sample. Suppose that one quarter of the data were reserved for the test set. Random sampling would randomly select 25% for the test set and use the remainder for the training set. We can use the [`rsample`](https://rsample.tidymodels.org/) package for this purpose.

Since random sampling uses random numbers, it is important to set the random number seed. This ensures that the random numbers can be reproduced at a later time (if needed).

The function `rsample::initial_split()` takes the original data and saves the information on how to make the partitions. In the original analysis, the authors made their own training/test set and that information is contained in the column `case`. To demonstrate how to make a split, we’ll remove this column before we make our own split:

```{r}
set.seed(123)
cell_split <- initial_split(cells |>  
                            select(-case),
                            prop = 3/4,
                            strata = class)
```

Here we used the [`strata`](https://rsample.tidymodels.org/reference/initial_split.html) argument, which conducts a stratified split. This ensures that, despite the imbalance we noticed in our `class` variable, our training and test data sets will keep roughly the same proportions of poorly and well-segmented cells as in the original data. After the `initial_split`, the `training()` and `testing()` functions return the actual data sets.

```{r}
cell_train <- training(cell_split)
cell_test  <- testing(cell_split)

nrow(cell_train)

nrow(cell_train)/nrow(cells)


# training set proportions by class
cell_train %>% 
  count(class) %>% 
  mutate(prop = n/sum(n))

# test set proportions by class
cell_test %>% 
  count(class) %>% 
  mutate(prop = n/sum(n))
```

The majority of the modeling work is then conducted on the training set data.

## MODELING

[Random forest models](https://en.wikipedia.org/wiki/Random_forest) are [ensembles](https://en.wikipedia.org/wiki/Ensemble_learning) of [decision trees](https://en.wikipedia.org/wiki/Decision_tree. A large number of decision tree models are created for the ensemble based on slightly different versions of the training set. When creating the individual decision trees, the fitting process encourages them to be as diverse as possible. The collection of trees are combined into the random forest model and, when a new sample is predicted, the votes from each tree are used to calculate the final predicted value for the new sample. For categorical outcome variables like class in our cells data example, the majority vote across all the trees in the random forest determines the predicted class for the new sample.

One of the benefits of a random forest model is that it is very low maintenance; it requires very little preprocessing of the data and the default parameters tend to give reasonable results. For that reason, we won’t create a recipe for the `cells` data.

At the same time, the number of trees in the ensemble should be large (in the thousands) and this makes the model moderately expensive to compute.

To fit a random forest model on the training set, let’s use the [`parsnip`](https://parsnip.tidymodels.org/) package with the [`ranger`](https://cran.r-project.org/web/packages/ranger/index.html) engine. We first define the model that we want to create:

```{r}
rf_mod <- 
  rand_forest(trees = 1000) |>  
  set_engine("ranger") |>  
  set_mode("classification")

```

