---
title: "Linear Models & `tidymodels`"
date: last-modified
date-format: "[Last modified on] MMMM DD, YYYY HH:mm:ss zzz"
format: 
  html: default
  pdf: default
editor: source
---

```{r include = FALSE}
library(knitr) # packages
library(tidyverse)
library(scales)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", comment = NA, warning = FALSE, message = FALSE)
```

**tidymodels** does not natively support forward selection because it prefers alternative regularization and feature selection methods like Lasso. You would need to implement forward selection by manually adding predictor variables to your model recipe within tidymodels or use an alternative R package designed for stepwise regression.

Why tidymodels doesn't support forward selection:

  * Preference for Regularization: tidymodels developers and the broader tidymodels community consider regularization methods (like Lasso) to be superior to stepwise selection for feature selection. 

  * "Locally Optimal" Nature: Forward selection is a "locally optimal" approach, meaning it finds the best model at each step but doesn't guarantee the globally optimal model. Regularization methods provide a more democratic way to handle coefficients. 
  
  * Goes Against Tidy Philosophy: Manually controlling the stepwise selection process in tidymodels can feel like it goes against the framework's philosophy of a streamlined, data-driven workflow.
  
How to perform feature selection with tidymodels:

  1. Use Regularization: Fit a regularized model like Lasso using parsnip, and the regularization process will perform feature selection by shrinking some coefficients to zero. 
  
  2. Manual Selection: You can manually select features by defining a recipe with the chosen predictors using recipes and then train the model with that specific recipe. 
  
  3. Use External Packages: For true forward selection, you would use other R packages that specifically offer stepwise regression, such as the StepReg package on CRAN. 
  
The **tidymodels** philosophy

The `tidymodels` framework is designed around a more consistent and robust approach to modeling than traditional stepwise procedures. Key aspects of this philosophy that lead away from forward selection include:

* Encourages superior methods: The developers prioritize methods like regularization, which are more stable and produce better predictive models than automated stepwise selection.

* Consistency: A core tenet of tidymodels is to have a uniform interface across different modeling engines. Stepwise procedures, which involve fitting many different model formulas, do not fit neatly into this framework.

* Reduced overfitting: Automatic stepwise selection can lead to models that overfit the training data. Regularization techniques like Lasso are designed to prevent this by shrinking coefficients toward zero or setting them to exactly zero, resulting in sparser and more stable models.  

### Recommended alternatives to forward selection
Instead of stepwise methods, the `tidymodels` framework provides several powerful and well-integrated tools for handling feature selection and model complexity.

#### Regularization with Lasso
This is the main alternative recommended by the `tidymodels` developers. Lasso regression performs variable selection by shrinking some coefficients all the way to zero.

* Model specification: Use `linear_reg()` with the mixture argument set to 1.

* Engine: Specify the `glmnet` engine, which is highly efficient for fitting many values of the penalty simultaneously.

* Tuning: Use `tune_grid()` to find the optimal value of the penalty hyperparameter.

* Workflow: The entire process is integrated smoothly into a tidymodels workflow, from preprocessing with recipes to resampling with rsample and tuning with tune. 

Example R code using Lasso:
[Emil Hvitfeldt](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/06-regularization.html) provides example R code using Lasso for feature selection with tidymodels. 
 
# Working with Data

```{r}
library(tidymodels)
library(MASS)
set.seed(3178)
boston_split <- initial_split(data = Boston, prop = 0.80)
boston_training <- training(boston_split)
boston_testing <- testing(boston_split)
```

## Visualizing and Checking Data

```{r}
#| label: "g1"
#| cache: true
library(MASS)
library(GGally)
ggpairs(data = boston_training,
        columns = c("medv", "dis", "lstat", "nox", "age", "rm"),
        aes(alpha = 0.01)) +
  
  theme_bw()
```


```{r}
boston_recipe <- 
  recipe(formula = medv ~ ., data = boston_training) |> 
  step_BoxCox(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors()) 
boston_recipe
```

```{r}
tidy(boston_recipe)
boston_rec_trained <- prep(boston_recipe, boston_training)
boston_rec_trained
```
```{r}
boston_trained_data <- bake(boston_rec_trained, new_data = NULL)
boston_trained_data
```
```{r}
#| label: "g2"
#| cache: true
ggpairs(data = boston_trained_data,
        columns = c("medv", "dis", "lstat", "nox", "age", "rm"),
        aes(alpha = 0.01)) +
  
  theme_bw()
```

## Resampling

Goal is to predict `medv`.

```{r}
lm_spec <- linear_reg() |> 
  set_mode("regression") |> 
  set_engine("lm")
```

First we will ignore the previously normalized and transformed variables and use the raw data.  Then, we will go back and use the previous `boston_recipe` with the transormations and normaliztions to see how the models compare.

```{r}
lm_fit <- lm_spec |> 
  fit(medv ~ ., data = boston_training)
```

```{r}
augment(lm_fit, new_data = boston_testing)
augment(lm_fit, new_data = boston_testing) |> 
  rmse(truth = medv, estimate = .pred) -> T1
T1
```

Compare to the training RMSE:

```{r}
augment(lm_fit, new_data = boston_training) |> 
  rmse(truth = medv, estimate = .pred) -> T2
T2
  
```
Note that we expect the training data to provide and overly optimistic estimate of the RMSE which in fact is what happens in this case where we see the $\text{RMSE}_{\text{Train}} = `r T2$.estimate`$ and $\text{RMSE}_{\text{Test}} = `r T1$.estimate`$.

Consider what happens when we preprocess the data using `recipe()`.

```{r}
boston_recipe <- 
  recipe(formula = medv ~ ., data = boston_training) |> 
  step_BoxCox(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors()) 
boston_wkfl <- workflow() |> 
  add_recipe(boston_recipe) |> 
  add_model(lm_spec)
boston_wkfl
```


```{r}
boston_fit <- fit(boston_wkfl, data = boston_training)
augment(boston_fit, new_data = boston_testing) |> 
  metrics(truth = medv, estimate = .pred)
augment(boston_fit, new_data = boston_training) |> 
  metrics(truth = medv, estimate = .pred)
```
Consider what happens if the seed is changed with the `initial_split()`.

```{r}
set.seed(3)
boston_split <- initial_split(data = Boston, prop = 0.80)
boston_training <- training(boston_split)
boston_testing <- testing(boston_split)
lm_fit <- lm_spec |> 
  fit(medv ~ ., data = boston_training)
augment(lm_fit, new_data = boston_testing)
augment(lm_fit, new_data = boston_testing) |> 
  rmse(truth = medv, estimate = .pred) -> T3
T3
augment(lm_fit, new_data = boston_training) |> 
  rmse(truth = medv, estimate = .pred) -> T4
T4
boston_recipe <- 
  recipe(formula = medv ~ ., data = boston_training) |> 
  step_BoxCox(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors()) 
boston_wkfl <- workflow() |> 
  add_recipe(boston_recipe) |> 
  add_model(lm_spec)
boston_fit <- fit(boston_wkfl, data = boston_training)
augment(boston_fit, new_data = boston_testing) |> 
  rmse(truth = medv, estimate = .pred)
augment(boston_fit, new_data = boston_training) |> 
  rmse(truth = medv, estimate = .pred)
```




## k-Fold Cross-Validation

```{r}
#| label: "cv103"
#| cache: true
set.seed(23)
boston_folds <- vfold_cv(boston_training, v = 10, repeats = 3)
boston_folds

boston_wkfl |> 
  fit_resamples(
    resamples = boston_folds,
    metrics = metric_set(rmse, mae, rsq),
    control = control_resamples(save_pred = TRUE)
  ) -> tenbyfive
tenbyfive |> collect_predictions() |> 
  metrics(truth = medv, estimate = .pred) 
```

## Ridge Regression

```{r}
ridge_spec <- linear_reg(mixture = 0, penalty = 0) |> 
  set_mode("regression") |> 
  set_engine("glmnet")
ridge_fit <- fit(ridge_spec, medv ~ ., data = boston_training)
tidy(ridge_fit)
```


Consider what happens as the penalty increases in the graph below.

```{r}
ridge_fit |> 
  autoplot() +
  theme_bw()
```

```{r}
predict(ridge_fit, new_data = boston_training, penalty = 1)
predict(ridge_fit, new_data = boston_training, penalty = 10)
predict(ridge_fit, new_data = boston_training, penalty = 100)
predict(ridge_fit, new_data = boston_training, penalty = 1000)
```
So how to we find the "best" `penalty` (`penalty` in tidymodels $\equiv \lambda$, note: $\lambda = \text{penalty} = 0 \text{and} \alpha = \text{mixture} = 0$ is normal regression)?  We can use tuning to find the best hyperparameter.  For now, we will only consider a grid search with evenly spaced parameter values to optimize our model.

```{r}
ridge_recipe <-
  recipe(formula = medv ~ ., data = boston_training) |> 
  step_BoxCox(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors())

ridge_spec <- 
  linear_reg(penalty = tune(), mixture = 0) |> 
  set_mode("regression") |> 
  set_engine("glmnet")

ridge_wf <- workflow() |> 
  add_recipe(ridge_recipe) |> 
  add_model(ridge_spec)
```

We need values of the `penalty`.  Looking at the graph from `autoplot()`, we select values from 0.001 to 1000 using `grid_regular()` which will create a grid of evenly spaced values.

```{r}
penalty_grid <- grid_regular(penalty(range = c(-3, 3)), levels = 40)
penalty_grid
```
```{r}
#| label: "ridgetune"
#| cache: true
tune_result <- tune_grid(
  ridge_wf,
  resamples = boston_folds,
  grid = penalty_grid
)
tune_result
```

```{r}
autoplot(tune_result)
```
```{r}
collect_metrics(tune_result)
```
```{r}
best_penalty <- select_best(tune_result, metric = "rmse")
best_penalty
```
```{r}
ridge_final <- finalize_workflow(ridge_wf, best_penalty)
ridge_final_fit <- fit(ridge_final, data = boston_training)
augment(ridge_final_fit, new_data = boston_testing) |> 
  metrics(truth = medv, estimate = .pred)
```
## The LASSO

To specify the LASSO use $\alpha = \text{mixture} = 1$.  Recall that $\text{penalty} = \lambda$.

```{r}
lasso_recipe <-
  recipe(formula = medv ~ ., data = boston_training) |> 
  step_BoxCox(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors())

lasso_spec <- 
  linear_reg(penalty = tune(), mixture = 1) |> 
  set_mode("regression") |> 
  set_engine("glmnet")

lasso_wf <- workflow() |> 
  add_recipe(lasso_recipe) |> 
  add_model(lasso_spec)
```

```{r}
penalty_grid <- grid_regular(penalty(range = c(-2, 2)), levels = 10)
```

```{r}
#| label: "lassotune"
#| cache: true
tune_res <- tune_grid(
  lasso_wf,
  resamples = boston_folds, 
  grid = penalty_grid
)

autoplot(tune_res) + 
  theme_bw()
```

```{r}
best_penalty <- select_best(tune_res, metric = "rmse")
best_penalty
lasso_final <- finalize_workflow(lasso_wf, best_penalty)
lasso_final_fit <- fit(lasso_final, data = boston_training)
augment(lasso_final_fit, new_data = boston_testing) |> 
  metrics(truth = medv, estimate = .pred)
```

## Elastic net

```{r}
#| label: "enettune"
#| cache: true
enet_recipe <-
  recipe(formula = medv ~ ., data = boston_training) |> 
  step_BoxCox(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors())

enet_spec <- 
  linear_reg(penalty = tune(), mixture = tune()) |> 
  set_mode("regression") |> 
  set_engine("glmnet")

enet_wf <- workflow() |> 
  add_recipe(enet_recipe) |> 
  add_model(enet_spec)

penalty_grid <- grid_regular(penalty(range = c(-2, 2)), mixture(range = c(0, 1)), levels = 10)

tune_res <- tune_grid(
  enet_wf,
  resamples = boston_folds, 
  grid = penalty_grid
)
```

```{r}
best_penalty <- select_best(tune_res, metric = "rmse")
best_penalty
enet_final <- finalize_workflow(enet_wf, best_penalty)
enet_final_fit <- fit(enet_final, data = boston_training)
augment(enet_final_fit, new_data = boston_testing) |> 
  metrics(truth = medv, estimate = .pred)
```

