---
title: "Not `mtcars` AGAIN"
date: last-modified
date-format: "[Last modified on] MMMM DD, YYYY HH:mm:ss zzz"
format: 
  html: default
  pdf: default
editor: source
---

```{r include = FALSE}
library(knitr) # packages
library(tidyverse)
library(scales)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", comment = NA, message = FALSE,  warning = FALSE)
```

:::{.callout-note title="Crediting the materials" icon=false}
The majority of the material in the first part of this document is taken from the free online course at [https://supervised-ml-course.netlify.app/](https://supervised-ml-course.netlify.app/) written by Julia Silge.
:::

In this first case study, you will predict fuel efficiency from a US Department of Energy data set for real cars of today.

In this case study, you will predict the fuel efficiency ‚õΩ of modern cars from characteristics of these cars, like transmission and engine displacement. Fuel efficiency is a numeric value that ranges smoothly from about 15 to 40 miles per gallon. To predict fuel efficiency you will build a **Regression** model.


## Visualize the fuel efficiency distribution

The first step before you start modeling is to explore your data. In this course we'll practice using tidyverse functions for exploratory data analysis. Start off this case study by examining your data set and visualizing the distribution of fuel efficiency. The `ggplot2` package, with functions like `ggplot()` and `geom_histogram()`, is included in the `tidyverse`.  The `tidyverse` metapackage is loaded for you, so you can use `readr` and `ggplot2`.

```{r}
library(tidyverse)
cars2018 <- read_csv("../Data/cars2018.csv")
```


* Take a look at the `cars2018` object using `glimpse()`.

```{r}
# Print the cars2018 object
glimpse(cars2018)
```

* Use the appropriate column from `cars2018` in the call to `aes()` so you can plot a histogram of fuel efficiency (miles per gallon, mpg).  Set the correct `x` and `y` labels.

```{r}
# Plot the histogram
ggplot(cars2018, aes(x = mpg)) +
  geom_histogram(bins = 25, color = "black", fill = "red") +
  labs(x = "Fuel efficiency (mpg)",
       y = "Number of cars") + 
  theme_bw()
```

```{r}
# Consider using log10(mpg) instead of mpg
cars2018 <- cars2018 |> 
  mutate(log_mpg = log(mpg))
ggplot(cars2018, aes(x = log_mpg)) +
  geom_histogram(bins = 15, color = "black", fill = "red") +
  labs(x = "Fuel efficiency (log_mpg)",
       y = "Number of cars") + 
  theme_bw()
```



## Build a simple linear model

Before embarking on more complex machine learning models, it‚Äôs a good idea to build the simplest possible model to get an idea of what is going on. In this case, that means fitting a simple linear model using base R's `lm()` function.

### Instructions {-}

* Use `select()` to deselect the two columns `model` and `model_index` from `cars2018`; these columns tell us the individual identifiers for each car and it would not make sense to include them in modeling.  Store the results in `car_vars`.

```{r}
# Deselect the 2 columns to create cars_vars
car_vars <- cars2018 |> 
    select(-model, -model_index)
```

* Fit `mpg` as the predicted quantity, explained by all the predictors, i.e., `.` in the R formula input to `lm()`. Store the linear model object in `fit_all`. (You may have noticed the log distribution of MPG in the last exercise, but don‚Äôt worry about fitting the logarithm of fuel efficiency yet.)

```{r}
# Fit a linear model
fit_all <- lm(mpg ~ . - log_mpg, data = car_vars)
```


* Print the `summary()` of the `model`fit_all`.

```{r}
# Print the summary of the model
summary(fit_all)
# Better yet
broom::tidy(fit_all) |> 
  knitr::kable()
# and
broom::glance(fit_all) |> 
  knitr::kable()
```

You just performed some exploratory data analysis and built a simple linear model using base R's `lm()` function.

## Getting started with `tidymodels`

### Training and testing data

Training models based on all of your data at once is typically not a good choice. üö´ Instead, you can create subsets of your data that you use for **different purposes**, such as **training** your model and then **testing** your model.

Creating training/testing splits reduces **overfitting**. When you evaluate your model on data that it was not trained on, you get a better estimate of how it will perform on new data.

### Instructions

* Load the `tidymodels` metapackage, which also includes `dplyr` for data manipulation.

```{r}
# Load tidymodels
library(tidymodels)
```

* Create a data split that divides the original data into 80%/20% sections and (roughly) evenly divides the partitions between the different types of `transmission`.  Assign the 80% partition to `car_train` and the 20% partition to `car_test`.

```{r}
# Split the data into training and test sets
set.seed(1234)
car_split <- car_vars |> 
    select(-mpg) |> 
    initial_split(prop = 0.8, strata = transmission)

car_train <- training(car_split)
car_test <- testing(car_split)

glimpse(car_train)
glimpse(car_test)
```

## Train models with `tidymodels`

Now that your `car_train` data is ready, you can fit a set of models with `tidymodels`. When we model data, we deal with model type (such as linear regression or random forest), mode (regression or classification), and model engine (how the models are actually fit). In `tidymodels`, we capture that modeling information in a model **specification**, so setting up your model specification can be a good place to start. In these exercises, fit one linear regression model and one random forest model, without any resampling of your data.

### Instructions

* Fit a basic linear regression model to your `car_train` data.  (Notice that we are fitting to `log(mpg)` since the fuel efficiency had a log normal distribution.)

```{r}
# Build a linear regression model specification
lm_spec <- linear_reg() |> 
    set_engine("lm")

# Train a linear regression model
fit_lm <- lm_spec |> 
    fit(log_mpg ~ ., data = car_train)

# Print the model object
broom::tidy(fit_lm) |> 
  knitr::kable()
# and
broom::glance(fit_lm) |> 
  knitr::kable()
```

* Fit a random forest model to your `car_train` data.

```{r}
# Build a random forest model specification
rf_spec <- rand_forest() |> 
    set_engine("ranger", importance = "impurity") |> 
    set_mode("regression")

# Train a random forest model
fit_rf <- rf_spec |> 
    fit(log_mpg ~ ., data = car_train)

# Print the model object
fit_rf
```
```{r}
vip::vip(fit_rf) +
  theme_bw()
```



## Evaluate model performance

The `fit_lm` and `fit_rf` models you just trained are in your environment. It's time to see how they did! ü§© How are we doing do this, though?! ü§î There are several things to consider, including both what metrics and what data to use.

For regression models, we will focus on evaluating using the root mean squared error metric. This quantity is measured in the same units as the original data (log of miles per gallon, in our case). Lower values indicate a better fit to the data. It's not too hard to calculate root mean squared error manually, but the [yardstick](https://yardstick.tidymodels.org/) package offers convenient functions for this and many other model performance metrics.

### Instructions

Note: The `yardstick` package is loaded since it is one of the packages in `tidyverse`.

* Create new columns for model predictions from each of the models you have trained, first linear regression and then random forest.

```{r}
# Create the new columns
results <- car_train |> 
    bind_cols(predict(fit_lm, car_train) |> 
                  rename(.pred_lm = .pred)) |> 
    bind_cols(predict(fit_rf, car_train) |> 
                  rename(.pred_rf = .pred)) |> 
    relocate(log_mpg, .pred_lm, .pred_rf, .before = displacement)
head(results) |> 
  knitr::kable()
```

* Evaluate the performance of these models using [`metrics()`](https://yardstick.tidymodels.org/reference/metrics.html) by specifying the column that contains the real fuel efficiency.

```{r}
# Evaluate the performance
metrics(results, truth = log_mpg, estimate = .pred_lm) |> 
  knitr::kable()
metrics(results, truth = log_mpg, estimate = .pred_rf) |> 
  knitr::kable()
```

## Use the testing data

‚ÄúBut wait!‚Äù you say, because you have been paying attention. ü§î ‚ÄúThat is how these models perform on the **training** data, the data that we used to build these models in the first place.‚Äù This is not a good idea because when you evaluate on the same data you used to train a model, the performance you estimate is too optimistic.

Let‚Äôs evaluate how these simple models perform on the **testing** data instead.

```{r}
# Create the new columns
results <- car_test  |> 
    bind_cols(predict(fit_lm, car_test)  |> 
                  rename(.pred_lm = .pred))  |> 
    bind_cols(predict(fit_rf, car_test)  |> 
                  rename(.pred_rf = .pred)) |> 
    relocate(log_mpg, .pred_lm, .pred_rf, .before = displacement)

# Evaluate the performance
metrics(results, truth = log_mpg, estimate = .pred_lm) |> 
  knitr::kable()
metrics(results, truth = log_mpg, estimate = .pred_rf) |> 
  knitr::kable()
```


You just trained models one time on the whole training set and then evaluated them on the testing set. Statisticians have come up with a slew of approaches to evaluate models in better ways than this; many important ones fall under the category of **resampling**.

The idea of resampling is to create simulated data sets that can be used to estimate the performance of your model, say, because you want to compare models. You can create these resampled data sets instead of using either your training set (which can give overly optimistic results, especially for powerful ML algorithms) or your testing set (which is extremely valuable and can only be used once or at most twice).

The first resampling approach we're going to try in this course is called the bootstrap. Bootstrap resampling means drawing **with replacement** from our original dataset and then fitting on that dataset.

Let's think about...cars! üöóüöåüöôüöï

## Bootstrap resampling

In the last set of exercises, you trained linear regression and random forest models without any resampling. Resampling can help us evaluate our machine learning models more accurately.

Let's try bootstrap resampling, which means creating data sets the same size as the original one by randomly drawing **with replacement** from the original. In `tidymodels`, the default behavior for bootstrapping is 25 resamplings, but you can change this using the `times` argument in `bootstraps()` if desired.

### Instructions

* Create bootstrap resamples to evaluate these models. The function to create this kind of resample is `bootstraps()`.

```{r}
## Create bootstrap resamples
set.seed(444)
car_boot <- bootstraps(data = car_train, times = 25)
```

* Evaluate both kinds of models, the linear regression model and the random forest model.

```{r}
#| label: "bothmodels"
#| cache: true
# Evaluate the models with bootstrap resampling
lm_res <- lm_spec |> 
    fit_resamples(
        log_mpg ~ .,
        resamples = car_boot,
        control = control_resamples(save_pred = TRUE)
    )

rf_res <- rf_spec |> 
    fit_resamples(
        log_mpg ~ .,
        resamples = car_boot,
        control = control_resamples(save_pred = TRUE)
    )
```

## Plot modeling results

You just trained models on bootstrap resamples of the training set and now have the results in `lm_res` and `rf_res`. These results are available in your environment, trained using the training set. Now let's compare them.

Notice in this code how we use `bind_rows()` from `dplyr` to combine the results from both models, along with `collect_predictions()` to obtain and format predictions from each resample.

* First use `collect_predictions()` for the linear model.  Then use  `collect_predictions()` for the random forest model.

```{r}
results <-  bind_rows(lm_res |> 
                          collect_predictions() |> 
                          mutate(model = "lm"),
                      rf_res |> 
                          collect_predictions() |> 
                          mutate(model = "rf"))

glimpse(results)
```
* Show the bootstrapped results:

```{r}
results |> 
  group_by(model) |> 
  metrics(truth = log_mpg, estimate = .pred) |> 
  knitr::kable()
```


* Visualize the results:

```{r}
results |> 
  ggplot(aes(x = log_mpg, y = .pred)) +
  geom_abline(lty = "dashed", color = "gray50") +
  geom_point(aes(color = id), size = 1.5, alpha = 0.1, show.legend = FALSE) +
  geom_smooth(method = "lm") +
  facet_wrap(~ model) + 
  coord_obs_pred() + 
  theme_bw() + 
  labs(y = "Predicted (log_mpg)",
       x = "Actual (log_mpg)")
```

## Tune the Random Forest model

```{r}
# Build a random forest model specification
rf_spec <- rand_forest(mtry = tune(),
                       min_n = tune(),
                       trees = 1000) |> 
    set_engine("ranger", importance = "impurity") |> 
    set_mode("regression")
rf_spec
```

### Cross Validation

```{r}
set.seed(42)
car_folds <- vfold_cv(car_train, v = 10, repeats = 5)
```

```{r}
rf_recipe <- recipe(log_mpg ~.,  data = car_train) 
rf_wkfl <- workflow() |> 
  add_recipe(rf_recipe) |> 
  add_model(rf_spec)
rf_wkfl
```


### Tune the model

:::{.callout-tip}
The next code chunk will take 20 minutes or so to run.  Make sure to cache the chunk so that you can rebuild your document without having the code chunk run each time the document is rendered.
:::

```{r}
#| cache: true
#| label: "rf_tune"
doParallel::registerDoParallel()
set.seed(321)
rf_tune <- tune_grid(rf_wkfl, resample = car_folds, grid = 15)
rf_tune
```

```{r}
show_best(rf_tune, metric = "rmse")
```

```{r}
# rf_param <- tibble(mtry = 6, min_n = 2)
rf_param <- select_best(rf_tune, metric = "rmse")
rf_param
final_rf_wkfl <- rf_wkfl |> 
  finalize_workflow(rf_param)
final_rf_wkfl
```


```{r}
final_rf_fit <- final_rf_wkfl |> 
  fit(car_train)
# Variable importance plot
vip::vip(final_rf_fit) + 
  theme_bw()
```

```{r}
augment(final_rf_fit, new_data = car_test) |> 
  metrics(truth = log_mpg, estimate = .pred) -> RF
RF |> 
  knitr::kable()
```


```{r}
# R-squared plot
augment(final_rf_fit, new_data = car_test) |> 
  ggplot(aes(x = log_mpg, y = .pred)) + 
  geom_point() +
  geom_abline(lty = "dashed") +
  coord_obs_pred() + 
  theme_bw() + 
  labs(x = "Observed log_mpg",
       y = "Predicted log_mpg",
       title = "R-squared Plot")
```

## Using Boosting

```{r}
xgboost_spec <- 
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), 
             learn_rate = tune(), loss_reduction = tune(), 
             sample_size = tune()) |>  
  set_mode("regression") |>  
  set_engine("xgboost") 
xgboost_spec
```

```{r}
xgboost_recipe <- 
  recipe(formula = log_mpg ~  . , data = car_train) |> 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors())  |> 
  step_normalize(all_numeric_predictors()) |> 
  step_corr(all_numeric_predictors(), threshold = 0.9)
```


```{r}
xgboost_workflow <- 
  workflow() |>  
  add_recipe(xgboost_recipe) |>  
  add_model(xgboost_spec) 
xgboost_workflow
```

:::{.callout-tip}
Use `cache: true` as the next chunk takes a hot minute.
:::

```{r}
#| cache: true
#| label: "xgb_tune"
library(finetune) 
# used for tune_race_anova() which...
# after an initial number of resamples have been evaluated, 
# the process eliminates tuning parameter combinations that 
# are unlikely to be the best results using a repeated 
# measure ANOVA model.
set.seed(49)
xgboost_tune <-
  tune_race_anova(xgboost_workflow, resamples = car_folds, grid = 15)
xgboost_tune
```


```{r}
show_best(xgboost_tune, metric = "rmse")
```


```{r}
# xgboost_param <- tibble(trees = 2000, 
#                        min_n = 9, 
#                        tree_depth = 6, 
#                        learn_rate = 0.00681,
#                        loss_reduction = 0.0000000155, 
#                        sample_size = 0.771)
xgboost_param <- select_best(xgboost_tune)
final_xgboost_wkfl <- xgboost_workflow |> 
  finalize_workflow(xgboost_param)
final_xgboost_wkfl
```

```{r}
final_xgboost_fit <- final_xgboost_wkfl |> 
  fit(car_train)

augment(final_xgboost_fit, new_data = car_test) |> 
  metrics(truth = log_mpg, estimate = .pred) -> R5
R5 |> 
  knitr::kable()
```


## Elastic net

```{r}
enet_spec <- linear_reg(penalty = tune()) |> 
  set_engine("glmnet") |> 
  set_mode("regression")
enet_spec
```


```{r}
enet_recipe <- 
  recipe(formula = log_mpg ~  . , data = car_train) |> 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> 
  step_zv(all_predictors())  |> 
  step_normalize(all_numeric_predictors()) |> 
  step_corr(all_numeric_predictors(), threshold = 0.9)
```

```{r}
enet_workflow <- 
  workflow() |>  
  add_recipe(enet_recipe) |>  
  add_model(enet_spec) 
enet_workflow
```

```{r}
library(finetune)
set.seed(49)
enet_tune <-
  tune_race_anova(enet_workflow, resamples = car_folds, grid = 15)
enet_tune
```

```{r}
show_best(enet_tune, metric = "rmse")
enet_param <- select_best(enet_tune, metric = "rmse")
final_enet_wkfl <- enet_workflow |> 
  finalize_workflow(enet_param)
final_enet_wkfl
```

```{r}
final_enet_fit <- final_enet_wkfl |> 
  fit(car_train)
```

```{r}
augment(final_enet_fit, new_data = car_test) |> 
  metrics(truth = log_mpg, estimate = .pred) -> R6
R6 |> 
  knitr::kable()
```

```{r}
# Print the model object
broom::tidy(final_enet_fit) |> 
  knitr::kable()
```

## Natural Splines and Interactions

```{r}
lm_spec <- linear_reg() |> 
  set_engine("lm")
ns_recipe <- recipe(log_mpg ~ ., data = car_train) |> 
  step_ns(displacement, cylinders, gears, deg_free = 5) |> 
  step_interact(~drive:transmission) |> 
  step_interact(~drive:recommended_fuel)
ns_wkfl <- workflow() |> 
  add_recipe(ns_recipe) |> 
  add_model(lm_spec)
ns_wkfl
```

```{r}
final_lm_fit <- ns_wkfl |> 
  fit(car_train)
```

```{r}
augment(final_lm_fit, new_data = car_test) |> 
  metrics(truth = log_mpg, estimate = .pred) -> R7
R7 |> 
  knitr::kable()
```

```{r}
broom::tidy(final_lm_fit) |> 
  knitr::kable()
```



