---
title: "Regression Trees from ISLR"
author: "Alan T. Arnholt"
date: 'Last knit on `r format(Sys.time(), "%B %d, %Y at %X")`'
output: 
  bookdown::html_document2:
    theme: yeti
    highlight: textmate
---



```{r label = "setup", include=FALSE}
knitr::opts_chunk$set(comment = NA, fig.align = 'center', fig.height = 7, fig.width = 7, 
                      prompt = TRUE, highlight = TRUE, tidy = FALSE, warning = FALSE, 
                      message = FALSE)
# Parallel Processing
library(doMC)
registerDoMC(cores = 12)
```

# Advantages and Disadvantages of Trees

## Pros

* Trees are very easy to explain to people.  In fact, they are even easier to explain than linear regression!

* Some people believe that decision trees more closely mirror human decision-making than do regression and classification approaches.

* Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).

* Trees can easily handle qualitative predictors without the need to create dummy variables (`model.matrix()`).

## Cons

* Trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches.

* Trees suffer from _high variance_.  This means if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different.  In contrast, a procedure with _low variance_ will yield similar results if applied repeatedly to distinct data sets.

However, by aggregating many decision trees, using methods like _bagging_, _random forests_, and _boosting_, the predictive performance of trees can be substantially improved.


* Bagging---Bootstrap aggregation---Bagging is a general-purpose procedure for reducing the variance of a statistical learning method.

# Training data set

```{r, label = "train"}
library(MASS)
library(rpart)
set.seed(8341)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
```

## Create `train1` and `test1` with `caret`

```{r}
library(caret)
set.seed(821)
trainIndex <- createDataPartition(y = Boston$medv,
                                  p = 0.50,
                                  list = FALSE,
                                  times = 1)
train1 <- Boston[trainIndex, ]
test1 <- Boston[-trainIndex, ]
dim(train1)
dim(test1)
```

### Stepwise selection with `caret`

```{r}
fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 3,
                          allowParallel = TRUE)
set.seed(34)
stepMod <- train(medv ~., 
              data = train1,
              method = "leapSeq",
              trControl = fitControl,
              verbose = FALSE)
stepMod
summary(stepMod)
summary(stepMod$finalModel)
coef(stepMod$finalModel, id = 3)
```

#### Test

```{r}
yhat <- predict(stepMod, newdata = test1)
RMSE <- sqrt(mean((test1$medv - yhat)^2))
RMSE
```


## Fitting a linear model

```{r}
modstep <- stepAIC(lm(medv ~ ., data = Boston, subset = train), k = log(253))
summary(modstep)
```
```{r}
yhat1 <- predict(modstep, newdata= Boston[-train, ])
boston_test1 <- Boston[-train, "medv"]
MSE1 <- mean((yhat1 - boston_test1)^2)
RMSE1 <- sqrt(MSE1)
c(MSE1, RMSE1)
```

Here we fit a regression tree to the `Boston` data set.  

```{r}
tree_boston <- rpart(medv ~ ., data = Boston, subset = train)
summary(tree_boston)
tree_boston1 <- rpart(medv ~ ., data = train1)
summary(tree_boston1)
```

```{r, fig.width = 7, fig.height = 7}
plot(tree_boston)
text(tree_boston, pretty = 0, use.n = TRUE, all = TRUE, cex = 0.7)
```

```{r}
library(tree)
tree_boston2 <- tree(medv ~ ., data = Boston, subset = train)
summary(tree_boston2)
plot(tree_boston2)
text(tree_boston2, pretty = 0, use.n = TRUE, all = TRUE, cex = 0.7)
```


Notice that the output of `summary()` indicates that five of the variables have been used in constructing the tree.  In the context of a regression tree, the deviance is simply the sum of the squared errors for the tree.  We now plot the tree with the package `partykit` which only works on trees created with the package `rpart` not `tree`.


```{r, fig.width = 12}
library(partykit)
plot(as.party(tree_boston))
plot(as.party(tree_boston1))
```

The variable `lstat` measures the percentage of individuals with lower socioeconomic status.  The tree indicates that lower values of `lstat` correspond to more expensive houses.  The tree predicts a median house price of $46,000 for larger homes in suburbs in which residents have high socioeconomic status (`rm >= 7.437` and `lstat < 9.545`).  Now we use the `cv.tree()` function to see whether pruning the tree will improve performance.

```{r}
set.seed(3)
cv_boston2 <- cv.tree(tree_boston2)
plot(cv_boston2$size, cv_boston2$dev, type = "b")
```
In this case, the most complex tree is selected by cross-validation.  However, if we wish to prune the tree, we could do so as follows, using the `prune.tree()` function:

```{r}
prune_boston2 <- prune.tree(tree_boston2, best = 5)
plot(prune_boston2)
text(prune_boston2, pretty = 0, all = TRUE, cex = 0.7)
```

In keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.

```{r}
yhat <- predict(tree_boston2, newdata = Boston[-train, ])
boston_test <- Boston[-train, "medv"]
plot(yhat, boston_test)
abline(0, 1)
MSE <- mean((yhat - boston_test)^2)
RMSE <- sqrt(mean((yhat - boston_test)^2))
c(MSE, RMSE)
```

In other words, the test MSE associated with the regression tree is `r round(MSE,4)`.  The square root of the MSE is `r round(RMSE,4)`, indicating that this model leads to test predictions that are within around $`r round(RMSE*1000,2)` of the true median home value for the suburb.

## Using `test1`

```{r}
yhat1 <- predict(tree_boston1, newdata = test1)
MSE <- mean((test1$medv - yhat1)^2)
RMSE <- sqrt(MSE)
c(MSE, RMSE)
```

## Random Forest

```{r}
library(randomForest)
set.seed(32)
mod_forest <- randomForest(medv ~ ., data = train1, ntree = 1000)
mod_forest
yhat_rf <- predict(mod_forest, newdata = test1)
MSE_test <- mean((test1$medv - yhat_rf)^2)
RMSE_test <- sqrt(MSE_test)
c(MSE_test, RMSE_test)
```

## Random Forest with `caret`

```{r}
fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 3,
                          allowParallel = TRUE)
set.seed(32)
mtry = c(2, 4, 8)
tunegrid <- expand.grid(.mtry = mtry)
mod_rfc <- train(medv ~ .,
                 data = train1,
                 method = "rf",
                 ntree = 1000,
                 tuneGrid = tunegrid,
                 trControl = fitControl)
mod_rfc
```

```{r}
yhat2 <- predict(mod_rfc, newdata = test1)
MSE_test <- mean((test1$medv - yhat2)^2)
RMSE_test <- sqrt(MSE_test)
c(MSE_test, RMSE_test)
```

# Using `caret` and `caretEnsemble`

This time, we will create a `training` set of roughly 80% of the values in `Boston`.

```{r}
library(caret)
set.seed(757)
trainIndex <- createDataPartition(y = Boston$medv,
                                  p = 0.80,
                                  list = FALSE)
training <- Boston[trainIndex, ]
testing <- Boston[-trainIndex, ]
dim(training)
dim(testing)
```

Consider the type of each variable.

```{r}
sapply(training, class)
str(training)
summary(training)
```

Let us take a quick look at the correlation between all of the predictors.

```{r}
cor(training[, -14])
```

There are a lot of correlated variables...

## Unimodal Data Visualizations

Getting a quick look at the distribution of all predictors.

```{r, fig.width = 12, fig.height = 6}
par(mfrow = c(2, 7))
for(i in 1:13){
  hist(training[, i], main = names(training)[i], col = "pink")
}
par(mfrow = c(1, 1))
```

```{r}
par(mfrow = c(4, 4))
for(i in 1:13){
  plot(density(training[, i]), main = names(training)[i], col = "blue")
}
par(mfrow = c(1, 1))
```

It appears that some variables follow an exponential distribution while a few others might be bimodal.  Consider boxplots of each predictor next.

```{r}
par(mfrow = c(4, 4))
for(i in 1:13){
  boxplot(training[, i], main = names(training)[i])
}
par(mfrow = c(1, 1))
```

## Multimodal Data Visualizations

```{r}
pairs(training[, -14]) # Yuck
```

```{r}
library(corrplot)
rs <- cor(training[, -14])
corrplot(rs, method = "circle")
```

### Trying Different Algorithms

```{r}
myControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 3,
                          allowParallel = TRUE)
library(caretEnsemble)
set.seed(32)
models <- caretList(log(medv) ~ .,
                    data = training,
                    trControl = myControl,
                    tuneList = list(
                      mod_lm = caretModelSpec(method = "lm"),
                      mod_svm = caretModelSpec(method = "svmRadial"),
                      mod_knn = caretModelSpec(method = "knn"),
                      mod_gbm = caretModelSpec(method = "gbm"),
                      mod_BIC = caretModelSpec(method = "lmStepAIC", k = log(407)),
                      mod_AIC = caretModelSpec(method = "lmStepAIC"),
                      mod_BE = caretModelSpec(method = "leapBackward"),
                      mod_FS = caretModelSpec(method = "leapForward"),
                      mod_SS = caretModelSpec(method = "leapSeq"),
                      mod_RF = caretModelSpec(method = "ranger"),
                      mod_EN = caretModelSpec(method = "glmnet", 
                                              tuneGrid = expand.grid(alpha = c(0, 0.5, 1), 
                                                                     lambda = seq(0.0001, 1, length = 20)))
                      
                    ))

models_pred <- lapply(models, predict, newdata = testing)
models_pred <- as.data.frame((models_pred))
models_pred <- exp(models_pred)
head(models_pred)
bwplot(resamples(models))
modelCor(resamples(models))
```

Let us pick on the three best algorithms (RF, svm, and gbm).

```{r}
set.seed(32)
models <- caretList(log(medv) ~ .,
                    data = training,
                    trControl = myControl,
                    tuneList = list(
                      mod_svm = caretModelSpec(method = "svmRadial"),
                      mod_gbm = caretModelSpec(method = "gbm"),
                      mod_RF = caretModelSpec(method = "ranger")
                    ))

models_pred <- lapply(models, predict, newdata = testing)
models_pred <- as.data.frame((models_pred))
models_pred <- exp(models_pred)
head(models_pred)
bwplot(resamples(models))
modelCor(resamples(models))
```

```{r}
set.seed(32)
models <- caretList(medv ~ .,
                    data = training,
                    trControl = myControl,
                    preProc = c("center", "scale", "BoxCox"),
                    tuneList = list(
                      # mod_svm = caretModelSpec(method = "svmRadial"),
                      mod_gbm = caretModelSpec(method = "gbm"),
                      mod_RF = caretModelSpec(method = "ranger")
                    ))

models_pred <- lapply(models, predict, newdata = testing)
models_pred <- as.data.frame((models_pred))
# models_pred <- exp(models_pred)
head(models_pred)
bwplot(resamples(models))
modelCor(resamples(models))
```

## Should tune (next course)

## Ensemble

```{r}
set.seed(31)
ensemble <- caretEnsemble(models, trControl = myControl)
summary(ensemble)
coef(ensemble$ens_model$finalModel)[-1]
####
sum(coef(ensemble$ens_model$finalModel)[-1])
####
BM <- as.matrix(models_pred)
X <- cbind(rep(1, dim(BM)[1]), BM)
BETA <- t(coef(ensemble$ens_model$finalModel))
dim(BETA)
ANS <- X %*% t(BETA)
head(ANS) 
MSE_test <- mean((testing$medv - ANS)^2)
RMSE_test <- sqrt(MSE_test)
c(MSE_test, RMSE_test)
```


## Trees for Classification

```{r}
url <- "https://assets.datacamp.com/production/course_3022/datasets/credit.csv"
GC <- read.csv(url)
library(caret)
set.seed(321)
trainIndex <- createDataPartition(y = GC$default,
                                  p = 0.80,
                                  list = FALSE,
                                  times = 1)
train80 <- GC[trainIndex, ]
test20 <- GC[-trainIndex, ]
dim(train80)
dim(test20)
```


```{r, fig.width = 12}
library(rpart)
library(partykit)
credit_model <- rpart(default ~ ., data = train80, method = "class")
plot(partykit::as.party(credit_model))
```


```{r, fig.width = 12}
rpart.plot::rpart.plot(x = credit_model, type = 5)
```

Evaluating the performance of the model with test set classification error.

```{r}
library(caret)
class_prediction <- predict(object = credit_model,  
                        newdata = test20,   
                        type = "class")  
                            
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = test20$default)  
```

Cross-validate a bagged tree model with `caret`

```{r}
# Specify the training configuration
ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 5,      # 5 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC

# Cross validate the credit model using "treebag" method; 
# Track AUC (Area under the ROC curve)
set.seed(1)  # for reproducibility
credit_caret_model <- train(default ~ .,
                            data = train80, 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl)

# Look at the model object
print(credit_caret_model)

# Inspect the contents of the model list 
names(credit_caret_model)

# Print the CV AUC
credit_caret_model$results[,"ROC"]
```

```{r}
# Generate predictions on the test set
pred <- predict(object = credit_caret_model, 
                newdata = test20,
                type = "prob")

# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
Metrics::auc(actual = ifelse(test20$default == "yes", 1, 0), 
                    predicted = pred[,"yes"])
```

## Random Forest with `GC` data

```{r}
# Train a Random Forest
set.seed(1)  # for reproducibility
credit_model <- train(default ~ .,
                        data = train80, 
                        method = "ranger",
                        metric = "ROC",
                        trControl = ctrl)
                             
# Print the model output                             
print(credit_model)
```

